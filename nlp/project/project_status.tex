\documentclass{article}

\usepackage{verbatim}
\usepackage{fullpage}
\usepackage{amsfonts}

\author{Frank Liberato, Thomas Torsney-Weir}
\title{Semantic Role Labeling}

%page formatting
\setlength{\parskip}{2.5ex plus 0.5ex minus 0.5ex}
\setlength{\parindent}{0ex}

\begin{document}

\maketitle

\section{Progress}
We have divided up the work on our system so that we may act independently
even though we're in the same office.  Frank is primarilly responsible for 
designing the genetic algorithm portion.  Tom is working on getting the 
classifiers set up and extracting the basic features of the algorithm.

We have finished implementing the genetic algorithm and the feature extraction
portion of the system.  We need to integrate the two pieces together so that
the GA can augment each feature vector with its scores.  We have a script to 
translate feature vectors into arff files so getting the data to weka is
simple.  

We also need to design the experiments.  We will set up a cross validation
suite for all our experiments.  We need to separate data into training and
test sets, etc.

\section{System Description}
We designed a five part system.  We extract simple features from the given
constituent tree.  We then prune away unlikely constituents.  The next step
is to add additional features based on the constituent from a genetic 
algorithm scorer which will be described below.  We then pass the final 
feature vector to a classifier which is trained to identify if a given
constituent is an argument.  We then pass the feature vector plus the output
of the classifier to another classifier which actually identifies the argument
type.  This is identical to the two-stage classifiers in the CoNLL-2005 
systems.

\subsection{Preprocessing}
We will preprocess the parse tree using Xue and Palmer's method of pruning
out unlikely argument constituents from the parse tree.

\subsection{Genetic Algorithm}
Rather than 'basis trees', which early testing showed to be insufficiently
expressive to represent interesting features, the GA now evolves short
programs to compute discriminating features.

Each program computes a function $f(C,V,T) \rightarrow \mathbb{N}$, where
$C$ is the constituent node, $V$ is the verb, and $T$ is the parse tree
for the entire sentence.  For a given $C,V,T$, the function produces
the corresponding feature value.

These programs are implemented in a simple language.  The language has
many primitives for walking trees, hashing values together, simple
arithemtic, and so on.

As an example, the following program computes a feature which is the
number of immediate children of the constituent:

\begin{verbatim}
WHLE 0 0 {
    INCR 1
}
OUTV 1
\end{verbatim}

{\tt CHLD} iterates over the immediate children of the given node ({\tt 0}
is the name of a register, which is initialzed to hold the constituent
argument given when the program is run).  {\tt INCR} increments the
value of the given register ({\tt 1} in this case).  Finally,
{\tt OUTV} outputs the value of the named register and halts execution.

Programs are scored based on the following formula:

\[S(p) = \frac{R_P}{R_P+W_P} \cdot \frac{min(I_P,I_A)}{I_A}\]

where:

\begin{enumerate}
\item $S(P)$ is the score of program $P$
\item $R_P$ is the number of test instances correctly classified by $P$
\item $W_P$ is the number of incorrectly classified test instances by $P$
\item $I_P$ is the {\it information content} of the classifications
generated by P
\item $I_A$ is the {\it information content} of the correct classifications
\end{enumerate}

The {\it information content} $I_X$ of a set of classifications $x$ is
given by:

\[I_X = -(log_2(\frac{Y_X}{Y_X + N_X}) + log_2(\frac{N_X}{Y_X + N_X}))\]

where $Y_X$ and $N_X$ are the number of $YES$ and $NO$ classifications
in $X$, respectively.

To build the training and test sets for each program, a random subset
of the entire training corpus is selected.  We are experimenting with
the best size for each subset.  Currently, we are using one quarter
of the whole corpus for each set.

\subsection{Feature Coding}
Here is a list of the training features used in the identifier.  For the 
testing features we removed any features stemming from the argument and 
the subcategorization frame label.  The only difference from the identifier
classifier input and the argument type classifier is that the argument type
classifier has the output from the identifier classifier.

\begin{description}
\item[Parse Tree:] The full parse tree for the constituent.
\item[GA Score:] A set of scores from the genetic algorithm for the given
                 constituent.
\item[Argument Head:] The head word of the argument.
\item[Argument Label:] The nonterminal label on the root of the parse tree
                       for the argument.
\item[Verb:] The verb for which we want to identify arguments.
\item[Subcategorization:] The subcatorization frame label for the verb.
\item[Path:] Path from the verb to the root of the argument constituent
\end{description}

\subsection{Classifiers}
We will use the two-stage labeling strategy that most of the CoNLL-2005 
systems used.  For each constituent we will first identify whether it is
an argument or not.  We will then feed this information into an identifier
to identify the argument type of the constituent.

We will use Support Vector Machines for both the identifying and labeling
classifiers.  The training of these classifiers is occurs after training
the GA.

\end{document}

